<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="Hny" href="http://hening25.gitee.io/rss.xml"><link rel="alternate" type="application/atom+xml" title="Hny" href="http://hening25.gitee.io/atom.xml"><link rel="alternate" type="application/json" title="Hny" href="http://hening25.gitee.io/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><meta name="keywords" content="java,git,cas"><link rel="canonical" href="http://hening25.gitee.io/2025/10/09/temp/2025-10-09-%E9%9D%A2%E7%BB%8F%E5%8F%82%E8%80%83/"><title>面经参考 - 技能工具 | Hening = Hny = 终有弱水替沧海 再无相思寄巫山</title><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"><meta name="generator" content="Hexo 6.3.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">面经参考</h1><div class="meta"><span class="item" title="创建时间：2025-10-09 00:00:00"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">发表于</span> <time itemprop="dateCreated datePublished" datetime="2025-10-09T00:00:00+08:00">2025-10-09</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span class="text">本文字数</span> <span>3.5k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span class="text">阅读时长</span> <span>3 分钟</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">Hening</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="https://s1.imagehub.cc/images/2023/11/16/fbf4bbd9b42c10e0a158c0b23b8e3dfc.png"></li><li class="item" data-background-image="https://s1.imagehub.cc/images/2023/11/16/142d2ce10548c990a335b356aaff4ed5.jpeg"></li><li class="item" data-background-image="https://s1.imagehub.cc/images/2023/11/16/6c5c8e1690b9eca82c0daf6006fb3915.jpeg"></li><li class="item" data-background-image="https://s1.imagehub.cc/images/2023/11/16/52951f4e869782759f3f03662bdba646.png"></li><li class="item" data-background-image="https://s1.imagehub.cc/images/2023/11/16/0ff60154861df19a87f84b195e29c651.jpeg"></li><li class="item" data-background-image="https://s1.imagehub.cc/images/2023/11/16/e98eedf3d4b0f34e6fe8b2eef41d00b2.jpeg"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">首页</a></span><i class="ic i-angle-right"></i> <span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/%E6%8A%80%E8%83%BD%E5%B7%A5%E5%85%B7/" itemprop="item" rel="index" title="分类于 技能工具"><span itemprop="name">技能工具</span></a><meta itemprop="position" content="1"></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="http://hening25.gitee.io/2025/10/09/temp/2025-10-09-%E9%9D%A2%E7%BB%8F%E5%8F%82%E8%80%83/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="Hening"><meta itemprop="description" content="终有弱水替沧海 再无相思寄巫山, 终有弱水替沧海 再无相思寄巫山"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Hny"></span><div class="body md" itemprop="articleBody"><p>面经参考</p><p><span id="more"></span></p><h1 id="面经参考"><a class="anchor" href="#面经参考">#</a> 面经参考</h1><h2 id="transformer"><a class="anchor" href="#transformer">#</a> Transformer</h2><h5 id="transformer-中的可训练-queries-keys-和-values-矩阵从哪儿来transformer-中为何会有-queries-keys-和-values-矩阵只设置-values-矩阵本身来求-attention-不是更简单吗"><a class="anchor" href="#transformer-中的可训练-queries-keys-和-values-矩阵从哪儿来transformer-中为何会有-queries-keys-和-values-矩阵只设置-values-矩阵本身来求-attention-不是更简单吗">#</a> Transformer 中的可训练 Queries、Keys 和 Values 矩阵从哪儿来？Transformer 中为何会有 Queries、Keys 和 Values 矩阵，只设置 Values 矩阵本身来求 Attention 不是更简单吗？</h5><details class="info"><summary></summary><div><p>Q、K、V 矩阵并非凭空而来，而是通过可学习的线性变换从输入数据中派生出来的。它们的分工合作是 Transformer 理解复杂依赖关系的精髓所在：</p><ul><li><strong>Q（查询）</strong> 负责主动 “提问”，寻找相关信息。</li><li><strong>K（键）</strong> 负责被动 “应答”，提供可被匹配的标识。</li><li><strong>V（值）</strong> 负责承载 “实质内容”，是被最终提取的信息。</li></ul><p>这种 “<strong>各司其职</strong>” 的设计，使得 Transformer 能够动态地、有区分地聚焦于输入序列中不同部分的信息，这是其强大表达能力的基础。如果只保留 V 矩阵，就等于放弃了这种灵活且强大的注意力筛选机制。</p></div></details><h5 id="transformer-的-feed-forward-层在训练的时候到底在训练什么"><a class="anchor" href="#transformer-的-feed-forward-层在训练的时候到底在训练什么">#</a> Transformer 的 Feed Forward 层在训练的时候到底在训练什么？</h5><details class="info"><summary></summary><div><p>Transformer 的 Feed Forward 层（FFN）在训练时，核心目标是学习如何对自注意力层输出的、已经融合了上下文信息的向量表示进行更深层次的<strong>非线性变换和特征加工</strong>。</p><p>通常由一个 “<strong>升维 - 激活 - 降维</strong>” 的流程构成。</p></div></details><h5 id="transformer-的-positional-encoding-是如何表达相对位置关系的位置信息在不同的encoder-的之间传递会丢失吗"><a class="anchor" href="#transformer-的-positional-encoding-是如何表达相对位置关系的位置信息在不同的encoder-的之间传递会丢失吗">#</a> Transformer 的 Positional Encoding 是如何表达相对位置关系的，位置信息在不同的 Encoder 的之间传递会丢失吗？</h5><details class="info"><summary></summary><div><p>利用经典正弦余弦编码的相对性原理，原版 Transformer 采用的正弦余弦编码公式如下：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mi>E</mi><mo stretchy="false">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mo separator="true">,</mo><mn>2</mn><mi>i</mi><mo stretchy="false">)</mo><mo>=</mo><mi>s</mi><mi>i</mi><mi>n</mi><mo stretchy="false">(</mo><mn>1000</mn><msup><mn>0</mn><mrow><mn>2</mn><mi>i</mi><mi mathvariant="normal">/</mi><msub><mi>d</mi><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow></msub></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">PE(pos,2i)=sin(10000^{2i/d_{pos}})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.13889em">P</span><span class="mord mathnormal" style="margin-right:.05764em">E</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mord mathnormal">o</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord">2</span><span class="mord mathnormal">i</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1.1879999999999997em;vertical-align:-.25em"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">i</span><span class="mord mathnormal">n</span><span class="mopen">(</span><span class="mord">1</span><span class="mord">0</span><span class="mord">0</span><span class="mord">0</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.9379999999999998em"><span style="top:-3.113em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span><span class="mord mtight">/</span><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.16454285714285716em"><span style="top:-2.357em;margin-left:0;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2818857142857143em"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mi>E</mi><mo stretchy="false">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mo separator="true">,</mo><mn>2</mn><mi>i</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo><mo>=</mo><mi>c</mi><mi>o</mi><mi>s</mi><mo stretchy="false">(</mo><mn>1000</mn><msup><mn>0</mn><mrow><mn>2</mn><mi>i</mi><mi mathvariant="normal">/</mi><msub><mi>d</mi><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow></msub></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">PE(pos,2i+1)=cos(10000^{2i/d_{pos}})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.13889em">P</span><span class="mord mathnormal" style="margin-right:.05764em">E</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mord mathnormal">o</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord">2</span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord">1</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1.1879999999999997em;vertical-align:-.25em"></span><span class="mord mathnormal">c</span><span class="mord mathnormal">o</span><span class="mord mathnormal">s</span><span class="mopen">(</span><span class="mord">1</span><span class="mord">0</span><span class="mord">0</span><span class="mord">0</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.9379999999999998em"><span style="top:-3.113em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span><span class="mord mtight">/</span><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.16454285714285716em"><span style="top:-2.357em;margin-left:0;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2818857142857143em"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p><p>其中 <em>pos</em> 是绝对位置，<em>i</em> 是维度索引，<em>d</em> 是模型维度。</p><p>位置信息会<strong>贯穿整个模型的深层，在不同 Encoder 层之间稳定传递</strong>。这主要得益于 Transformer 的两项核心设计：</p><ol><li><strong>残差连接</strong>：每个子层（自注意力层、前馈网络）的输出都包含其输入信息，而输入中已经融入了位置编码。通过残差连接，这些位置信息得以直接传递到下一层。</li><li><strong>逐层传递</strong>：每一层的输出都作为下一层的输入。因此，经过第一层 Encoder 处理后得到的表征，已经是一个<strong>融合了词义和位置信息的复合体</strong>。这个复合体作为输入进入第二层 Encoder，其内部包含的位置信息会继续参与第二层的计算，并影响其输出。位置编码的影响会随着数据在各层中的前向传播而持续下去。</li></ol></div></details><h5 id="transformer-中的-layer-normalization-蕴含的神经网络的假设是什么为何使用layer-norm-而不是-batch-normtransformer-是否有其它更好的-normalization-的实现"><a class="anchor" href="#transformer-中的-layer-normalization-蕴含的神经网络的假设是什么为何使用layer-norm-而不是-batch-normtransformer-是否有其它更好的-normalization-的实现">#</a> Transformer 中的 Layer Normalization 蕴含的神经网络的假设是什么？为何使用 Layer Norm 而不是 Batch Norm？Transformer 是否有其它更好的 Normalization 的实现？</h5><details class="info"><summary></summary><div><p>LayerNorm 的核心思想是<strong>对单个样本在某一层所有神经元节点的输出进行归一化</strong>。它基于一个重要的假设：<strong>一个样本在神经网络某一层中的所有特征维度的分布应该是稳定且相似的</strong>（可以理解为满足某种独立同分布特性）。</p><p>Transformer 选择 LayerNorm 而非 BatchNorm，主要基于以下几点考虑：</p><ol><li><strong>对变长序列的天然适应性</strong>：自然语言处理中，句子长度各不相同。为了批量处理，短句会被填充（Padding）到相同长度。LayerNorm<strong> 独立处理每个样本</strong>，完全不受批次内其他序列长度或填充位置的影响。而 BatchNorm 会计算批次内所有样本同一特征的均值和方差，填充位的零值会严重污染这些统计量，导致归一化效果不佳。</li><li><strong>训练稳定性和批次大小不敏感</strong>：Transformer 模型庞大，训练时有时会采用较小的批次大小（Mini-batch）。BatchNorm 的效果高度依赖于批次大小，小批次时其计算的统计量非常不准确。LayerNorm 的计算与批次大小无关，因此在各种规模的批次下都能提供稳定的归一化效果，保证了训练过程的稳健。</li><li><strong>与自注意力机制的并行计算模式匹配</strong>：Transformer 的自注意力机制是高度并行化的。LayerNorm 在每个样本上独立操作，非常适合这种并行计算架构，不会引入跨样本的依赖。而 BatchNorm 需要同步整个批次的统计信息，在并行计算环境中可能成为瓶颈。</li></ol></div></details><h5 id="请具体分析-transformer-的-embeddigns-层-attention-层和-feedforward-层的复杂度"><a class="anchor" href="#请具体分析-transformer-的-embeddigns-层-attention-层和-feedforward-层的复杂度">#</a> 请具体分析 Transformer 的 Embeddigns 层、Attention 层和 Feedforward 层的复杂度</h5><details class="info"><summary></summary><div><p><strong>1. Embeddings 层</strong></p><p>这一层负责将离散的 token ID 转换为连续的向量表示。</p><ul><li><strong>时间复杂度 O (n⋅d)</strong>：对于长度为 <em>n</em> 的序列，需要执行 <em>n</em> 次查表操作，每次获取一个 <em>d</em> 维的向量。这是一个线性操作，效率很高。</li><li><strong>空间复杂度 O (∣V∣⋅d)</strong>：需要存储一个大小为 ∣<em>V</em>∣×<em>d</em> 的嵌入矩阵，其中 ∣<em>V</em>∣是词表大小。在词汇表很大时，这部分参数会占据相当可观的内存。</li></ul><p><strong>2. Attention 层</strong></p><p>自注意力机制是 Transformer 的核心，也是计算开销最大的部分。</p><ul><li><strong>时间复杂度 O (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>n</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">n^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8141079999999999em;vertical-align:0"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8141079999999999em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>⋅d)</strong>：这源于计算注意力得分矩阵 <em><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">QK^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.035771em;vertical-align:-.19444em"></span><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:.07153em">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8413309999999999em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.13889em">T</span></span></span></span></span></span></span></span></span></span></span><em> 的步骤，该矩阵大小为 <em>n</em>×</em>n</em>。对于序列中的每个位置，都需要与其他所有位置计算关联度，因此复杂度是序列长度的平方次。这使得 Transformer 在处理长序列时（如长文档或高分辨率图像分块）计算成本急剧上升。</li><li><strong>空间复杂度 O (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>d</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">d^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8141079999999999em;vertical-align:0"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8141079999999999em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>)</strong>：主要来自生成 Q、K、V 的三个线性变换矩阵 <em>W**Q</em>,<em>W**K</em>,<em>W**V</em>，每个矩阵的大小是 <em>d</em>×<em>d**k</em>（通常 <em>d**k</em>=<em>d</em>）。相比之下，存储 <em>n</em>×<em>n</em> 的注意力矩阵本身是临时的空间开销。</li></ul><p><strong>3. Feedforward 层</strong></p><p>前馈网络独立地处理每个位置的表示。</p><ul><li><strong>时间复杂度 O (n⋅<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>d</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">d^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8141079999999999em;vertical-align:0"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8141079999999999em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>)</strong>：FFN 通常先将 <em>d</em> 维的输入投影到一个更高的维度 <em>d**ff</em>（例如 4<em>d</em>），再投影回 <em>d</em> 维。每个位置的计算复杂度是 <em>O</em>(<em>d</em>⋅<em>d**ff</em>)，对于 <em>n</em> 个位置就是 <em>O</em>(<em>n</em>⋅<em>d</em>⋅<em>d**ff</em>)，即 <em>O</em>(<em>n</em>⋅<em>d</em>2)。</li><li><strong>空间复杂度 *O*(*d*2)</strong>：由两层全连接层的权重矩阵决定，即 <em>W</em>1∈R<em>d</em>×<em>d**ff</em> 和 <em>W</em>2∈R<em>d**ff</em>×<em>d</em>。</li></ul></div></details><h5 id="transformer-中的神经网络为何能够很好的表示信息"><a class="anchor" href="#transformer-中的神经网络为何能够很好的表示信息">#</a> Transformer 中的神经网络为何能够很好的表示信息？</h5><details class="info"><summary></summary><div><p>Transformer 神经网络强大的信息表示能力源于其<strong>对全局上下文的动态感知</strong>、<strong>并行高效的计算架构</strong>以及<strong>通过深度堆叠实现的层次化抽象</strong>。它通过自注意力机制捕捉内部关联，通过位置编码理解顺序，通过前馈网络进行非线性变换，再通过残差连接和层归一化稳定地训练所有这些组件，从而使其能够学习到数据中极其复杂和抽象的模式。</p></div></details><h5 id="请描述-transformer-中的-tokenization-的数学原理-运行流程-问题及具体改进方法"><a class="anchor" href="#请描述-transformer-中的-tokenization-的数学原理-运行流程-问题及具体改进方法">#</a> 请描述 Transformer 中的 Tokenization 的数学原理、运行流程、问题及具体改进方法</h5><details class="info"><summary></summary><div><p>Tokenization 的数学本质是<strong>将连续文本离散化</strong>，通过构建一个从文本片段到整数索引的映射函数。其核心在于<strong>词汇表的构建算法</strong>。</p></div></details><h5 id="请描述一下你认为的把-self-attention-复杂度从-on2-降低到-on有效方案"><a class="anchor" href="#请描述一下你认为的把-self-attention-复杂度从-on2-降低到-on有效方案">#</a> 请描述一下你认为的把 self-attention 复杂度从 O (n2) 降低到 O (n) 有效方案.</h5><details class="info"><summary></summary><div><ol><li>低秩近似</li></ol><p>这类方法基于一个关键观察：<strong>自注意力矩阵往往是低秩的</strong>，即其信息可以由一个低维子空间有效捕获。</p><ul><li><strong>Linformer</strong>: 提出将原始的键（K）和值（V）矩阵（维度为 n×d）通过<strong>投影矩阵</strong>降维到 k×d（k &lt;&lt;n）。这使得计算注意力权重时的矩阵乘法变为 (n×d)・(d×k) = n×k，从而将复杂度降至 O (n)。关键在于，即使 k 远小于 n（如 256），也能在多项任务上达到与标准注意力相当的性能。</li></ul><ol start="2"><li>线性注意力 (Linear Attention)</li></ol><p>这类方法通过<strong>数学变换（核函数技巧）</strong> 改变计算顺序，避免计算 n×n 的注意力矩阵。</p><ul><li><strong>Performer</strong>: 使用一个<strong>核函数</strong>来近似标准的 softmax 注意力。其巧妙之处在于将计算重新表述为：首先计算键和值的某种特征映射的外积，然后用查询向量去乘这个结果。这样就将计算顺序从 (Q・Kᵀ)・V 转换为 Q・(Kᵀ・V)，从而将复杂度从 O (n²d) 降为 O (nd²)（当 d 为常数时，即为 O (n)）。</li></ul></div></details><h5 id="请分享一下至少三种提升-transformer-预测速度的具体的方法及其数学原理"><a class="anchor" href="#请分享一下至少三种提升-transformer-预测速度的具体的方法及其数学原理">#</a> 请分享一下至少三种提升 Transformer 预测速度的具体的方法及其数学原理</h5><details class="info"><summary></summary><div><ol><li>高效注意力机制：利用核函数对标准的 Softmax 注意力进行数学上的近似改写。通过改变计算顺序，先将键（K）和值（V）相乘，再与查询（Q）作用，从而将复杂度降至 <em>O</em>(<em>n</em>)。</li><li>模型量化：将模型参数（权重）和激活值从 32 位浮点数（FP32）转换为低精度整数（如 INT8 甚至 INT4），从而带来多方面的加速效益。</li><li>改进解码策略</li></ol></div></details></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">更新于</span> <time title="修改时间：2025-10-09 16:40:42" itemprop="dateModified" datetime="2025-10-09T16:40:42+08:00">2025-10-09</time> </span><span id="2025/10/09/temp/2025-10-09-面经参考/" class="item leancloud_visitors" data-flag-title="面经参考" title="阅读次数"><span class="icon"><i class="ic i-eye"></i> </span><span class="text">阅读次数</span> <span class="leancloud-visitors-count"></span> <span class="text">次</span></span></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/2025/08/15/2025-08-15-Leetcode/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;s1.imagehub.cc&#x2F;images&#x2F;2023&#x2F;11&#x2F;16&#x2F;b60b630ac478d2911b6c682866cf5d09.jpeg" title="Leetcode刷题记录"><span class="type">上一篇</span> <span class="category"><i class="ic i-flag"></i> 技能工具</span><h3>Leetcode刷题记录</h3></a></div><div class="item right"></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%9D%A2%E7%BB%8F%E5%8F%82%E8%80%83"><span class="toc-number">1.</span> <span class="toc-text">面经参考</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#transformer"><span class="toc-number">1.1.</span> <span class="toc-text">Transformer</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#transformer-%E4%B8%AD%E7%9A%84%E5%8F%AF%E8%AE%AD%E7%BB%83-queries-keys-%E5%92%8C-values-%E7%9F%A9%E9%98%B5%E4%BB%8E%E5%93%AA%E5%84%BF%E6%9D%A5transformer-%E4%B8%AD%E4%B8%BA%E4%BD%95%E4%BC%9A%E6%9C%89-queries-keys-%E5%92%8C-values-%E7%9F%A9%E9%98%B5%E5%8F%AA%E8%AE%BE%E7%BD%AE-values-%E7%9F%A9%E9%98%B5%E6%9C%AC%E8%BA%AB%E6%9D%A5%E6%B1%82-attention-%E4%B8%8D%E6%98%AF%E6%9B%B4%E7%AE%80%E5%8D%95%E5%90%97"><span class="toc-number">1.1.0.0.1.</span> <span class="toc-text">Transformer 中的可训练 Queries、Keys 和 Values 矩阵从哪儿来？Transformer 中为何会有 Queries、Keys 和 Values 矩阵，只设置 Values 矩阵本身来求 Attention 不是更简单吗？</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#transformer-%E7%9A%84-feed-forward-%E5%B1%82%E5%9C%A8%E8%AE%AD%E7%BB%83%E7%9A%84%E6%97%B6%E5%80%99%E5%88%B0%E5%BA%95%E5%9C%A8%E8%AE%AD%E7%BB%83%E4%BB%80%E4%B9%88"><span class="toc-number">1.1.0.0.2.</span> <span class="toc-text">Transformer 的 Feed Forward 层在训练的时候到底在训练什么？</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#transformer-%E7%9A%84-positional-encoding-%E6%98%AF%E5%A6%82%E4%BD%95%E8%A1%A8%E8%BE%BE%E7%9B%B8%E5%AF%B9%E4%BD%8D%E7%BD%AE%E5%85%B3%E7%B3%BB%E7%9A%84%E4%BD%8D%E7%BD%AE%E4%BF%A1%E6%81%AF%E5%9C%A8%E4%B8%8D%E5%90%8C%E7%9A%84encoder-%E7%9A%84%E4%B9%8B%E9%97%B4%E4%BC%A0%E9%80%92%E4%BC%9A%E4%B8%A2%E5%A4%B1%E5%90%97"><span class="toc-number">1.1.0.0.3.</span> <span class="toc-text">Transformer 的 Positional Encoding 是如何表达相对位置关系的，位置信息在不同的 Encoder 的之间传递会丢失吗？</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#transformer-%E4%B8%AD%E7%9A%84-layer-normalization-%E8%95%B4%E5%90%AB%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%81%87%E8%AE%BE%E6%98%AF%E4%BB%80%E4%B9%88%E4%B8%BA%E4%BD%95%E4%BD%BF%E7%94%A8layer-norm-%E8%80%8C%E4%B8%8D%E6%98%AF-batch-normtransformer-%E6%98%AF%E5%90%A6%E6%9C%89%E5%85%B6%E5%AE%83%E6%9B%B4%E5%A5%BD%E7%9A%84-normalization-%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.1.0.0.4.</span> <span class="toc-text">Transformer 中的 Layer Normalization 蕴含的神经网络的假设是什么？为何使用 Layer Norm 而不是 Batch Norm？Transformer 是否有其它更好的 Normalization 的实现？</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%AF%B7%E5%85%B7%E4%BD%93%E5%88%86%E6%9E%90-transformer-%E7%9A%84-embeddigns-%E5%B1%82-attention-%E5%B1%82%E5%92%8C-feedforward-%E5%B1%82%E7%9A%84%E5%A4%8D%E6%9D%82%E5%BA%A6"><span class="toc-number">1.1.0.0.5.</span> <span class="toc-text">请具体分析 Transformer 的 Embeddigns 层、Attention 层和 Feedforward 层的复杂度</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#transformer-%E4%B8%AD%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%BA%E4%BD%95%E8%83%BD%E5%A4%9F%E5%BE%88%E5%A5%BD%E7%9A%84%E8%A1%A8%E7%A4%BA%E4%BF%A1%E6%81%AF"><span class="toc-number">1.1.0.0.6.</span> <span class="toc-text">Transformer 中的神经网络为何能够很好的表示信息？</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%AF%B7%E6%8F%8F%E8%BF%B0-transformer-%E4%B8%AD%E7%9A%84-tokenization-%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86-%E8%BF%90%E8%A1%8C%E6%B5%81%E7%A8%8B-%E9%97%AE%E9%A2%98%E5%8F%8A%E5%85%B7%E4%BD%93%E6%94%B9%E8%BF%9B%E6%96%B9%E6%B3%95"><span class="toc-number">1.1.0.0.7.</span> <span class="toc-text">请描述 Transformer 中的 Tokenization 的数学原理、运行流程、问题及具体改进方法</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%AF%B7%E6%8F%8F%E8%BF%B0%E4%B8%80%E4%B8%8B%E4%BD%A0%E8%AE%A4%E4%B8%BA%E7%9A%84%E6%8A%8A-self-attention-%E5%A4%8D%E6%9D%82%E5%BA%A6%E4%BB%8E-on2-%E9%99%8D%E4%BD%8E%E5%88%B0-on%E6%9C%89%E6%95%88%E6%96%B9%E6%A1%88"><span class="toc-number">1.1.0.0.8.</span> <span class="toc-text">请描述一下你认为的把 self-attention 复杂度从 O (n2) 降低到 O (n) 有效方案.</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%AF%B7%E5%88%86%E4%BA%AB%E4%B8%80%E4%B8%8B%E8%87%B3%E5%B0%91%E4%B8%89%E7%A7%8D%E6%8F%90%E5%8D%87-transformer-%E9%A2%84%E6%B5%8B%E9%80%9F%E5%BA%A6%E7%9A%84%E5%85%B7%E4%BD%93%E7%9A%84%E6%96%B9%E6%B3%95%E5%8F%8A%E5%85%B6%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86"><span class="toc-number">1.1.0.0.9.</span> <span class="toc-text">请分享一下至少三种提升 Transformer 预测速度的具体的方法及其数学原理</span></a></li></ol></li></ol></li></ol></div><div class="related panel pjax" data-title="系列文章"><ul><li><a href="/2023/12/06/2023-12-06_%E7%89%B9%E5%BE%81%E5%9B%BE%E5%8F%AF%E8%A7%86%E5%8C%96/" rel="bookmark" title="特征图可视化">特征图可视化</a></li><li><a href="/2025/08/15/2025-08-15-Leetcode/" rel="bookmark" title="Leetcode刷题记录">Leetcode刷题记录</a></li><li class="active"><a href="/2025/10/09/temp/2025-10-09-%E9%9D%A2%E7%BB%8F%E5%8F%82%E8%80%83/" rel="bookmark" title="面经参考">面经参考</a></li></ul></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="Hening" data-src="/images/avatar.jpg"><p class="name" itemprop="name">Hening</p><div class="description" itemprop="description">终有弱水替沧海 再无相思寄巫山</div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">17</span> <span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">10</span> <span class="name">分类</span></a></div><div class="item tags"><a href="/tags/"><span class="count">10</span> <span class="name">标签</span></a></div></nav><div class="social"><span class="exturl item github" data-url="aHR0cHM6Ly9naXRodWIuY29tL0hueTEyMTY=" title="https:&#x2F;&#x2F;github.com&#x2F;Hny1216"><i class="ic i-github"></i></span> <span class="exturl item email" data-url="bWFpbHRvOmhlbmluZzI1QG1haWwyLnN5c3UuZWR1LmNu" title="mailto:hening25@mail2.sysu.edu.cn"><i class="ic i-envelope"></i></span></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item"><a href="/about/" rel="section"><i class="ic i-user"></i>关于</a></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>文章</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li></ul></li><li class="item"><a href="/amusement/" rel="section"><i class="ic i-magic"></i>摸鱼</a></li><li class="item"><a href="/logging/" rel="section"><i class="ic i-file"></i>日志</a></li></ul></div></div></div><ul id="quick"><li class="prev pjax"></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"></div><div class="status"><div class="copyright">&copy; 2010 – <span itemprop="copyrightYear">2025</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">Hening @ Hening</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i> </span><span title="站点总字数">216k 字</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-coffee"></i> </span><span title="站点阅读时长">3:17</span></div><div class="powered-by">基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"2025/10/09/temp/2025-10-09-面经参考/",favicon:{show:"Hening",hide:"(´Д｀)大変だ！"},search:{placeholder:"文章搜索",empty:"关于 「 ${query} 」，什么也没搜到",stats:"${time} ms 内找到 ${hits} 条结果"},valine:!0,copy_tex:!0,katex:!0,fancybox:!0,copyright:'复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="https://cdn.polyfill.io/v2/polyfill.js"></script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script></body></html>