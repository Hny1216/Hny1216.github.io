---
title: Tucker分解
math: true
fancybox: true
date: 2025-07-17
categories:
- [深度学习,工具]
tags: 
- [矩阵变换,tucker]
---

Tucker分解

<!-- more -->

# Tucker分解

------

## 什么是 Tucker 分解？

Tucker 分解是 **张量（tensor）分解** 的一种，是矩阵 SVD（奇异值分解）在更高阶上的推广。

它的核心思想是：

> 用一个小的 **核心张量** $\mathcal{G}$，以及几个矩阵将一个大的原始张量表示出来，达到**降维、压缩、解耦**的效果。

------

## 张量与模式乘积（mode-n product）

- **矩阵是二维张量**，例如：$A \in \mathbb{R}^{m \times n}$
- **张量是更高维的数组**，例如：
  - 三阶张量 $\mathcal{X} \in \mathbb{R}^{I \times J \times K}$

### 模式-n乘积（Mode-n product）

给定三阶张量 $\mathcal{X} \in \mathbb{R}^{I \times J \times K}$，和矩阵 $U \in \mathbb{R}^{L \times I}$，那么：

$$\mathcal{Y} = \mathcal{X} \times_1 U \in \mathbb{R}^{L \times J \times K}$$

这表示对第一个维度（mode-1）做矩阵变换。其本质是：

> 把张量的第1个维度的每个“切片”乘以 U

------

## Tucker 分解的数学定义

设有一个三阶张量：

$$\mathcal{X} \in \mathbb{R}^{I \times J \times K}$$

Tucker 分解将其表示为：

$$\mathcal{X} \approx \mathcal{G} \times_1 A \times_2 B \times_3 C$$

其中：

- $\mathcal{G} \in \mathbb{R}^{R_1 \times R_2 \times R_3}$：核心张量（压缩表示）
- $A \in \mathbb{R}^{I \times R_1}$、$B \in \mathbb{R}^{J \times R_2}$、$C \in \mathbb{R}^{K \times R_3}$：模式矩阵，控制每个维度的投影
- $\times_n$：表示对张量的第 n 维进行矩阵乘法

> 总结一句话：Tucker 分解就是：
>
> **“把高维张量压缩成小的核心张量 + 每一维的线性变换矩阵”**

------

## 图示理解（示意图）

```
            Tucker Decomposition
           ┌───────────────┐
           │  Tensor X     │         原始张量 X ∈ ℝ^{I×J×K}
           └────┬──────────┘
                ↓ Tucker分解
       ┌────────┴────────┐
       ↓        ↓        ↓
  Matrix A   Matrix B   Matrix C    模式矩阵（每一维的降维）
   (I×R1)     (J×R2)     (K×R3)
       ↓        ↓        ↓
            Core tensor G           核心张量 ∈ ℝ^{R1×R2×R3}
```

------

## 为什么 Tucker 有用？

Tucker 分解可以：

1. **降低维度/压缩张量**：原始张量参数量为 $I \times J \times K$，而 Tucker 分解后是：

   $$I \cdot R_1 + J \cdot R_2 + K \cdot R_3 + R_1 \cdot R_2 \cdot R_3$$

   通常远小于原始张量。

2. **捕捉模态之间的交互关系**：核心张量 $\mathcal{G} $建模了压缩后表示之间的高阶交互。

3. **在多模态场景中用于融合**：比如图像、文本输入，分别编码后送入 Tucker 融合模块。

------

## 举个数值例子

设有张量 $\mathcal{X} \in \mathbb{R}^{4 \times 3 \times 2}$，我们用 Tucker 分解它：

- 模式矩阵：
  - $A \in \mathbb{R}^{4 \times 2}$
  - $B \in \mathbb{R}^{3 \times 2}$
  - $C \in \mathbb{R}^{2 \times 1}$
- 核心张量 $\mathcal{G} \in \mathbb{R}^{2 \times 2 \times 1}$

合成的张量为：

$$\mathcal{X} \approx \mathcal{G} \times_1 A \times_2 B \times_3 C \in \mathbb{R}^{4 \times 3 \times 2}$$

------

## 与其他分解的对比

| 方法                   | 类型           | 适用场景              | 特点               |
| ---------------------- | -------------- | --------------------- | ------------------ |
| Tucker                 | 多模态张量分解 | 多模态融合            | 保留交互，参数可控 |
| CP (CANDECOMP/PARAFAC) | 张量秩分解     | 分解张量为秩1分量之和 | 更简单但可解释性低 |
| PCA                    | 矩阵分解       | 降维                  | 相当于 SVD 的特例  |
| SVD                    | 矩阵分解       | 特征提取              | 二阶张量特例       |

------

## 实际应用

### 1. 多模态融合

如在视觉问答中，将图像向量 vv 和文本向量 qq 融合，用 Tucker 分解实现有效的高阶交互建模。

### 2. 模型压缩

可以用 Tucker 分解对 CNN 卷积核或 Transformer 权重张量进行压缩。

### 3. 多任务学习

可以用共享核心张量，同时用不同模式矩阵适配不同任务。

------

## Tucker 分解的 PyTorch 简单实现

```python
# 简化版 Tucker 融合（v: image, q: text）
v_proj = U_v(v)      # [B, r1]
q_proj = U_q(q)      # [B, r2]
outer = torch.einsum("bi,bj->bij", v_proj, q_proj)   # [B, r1, r2]
fused = torch.einsum("bij,ijk->bk", outer, core)     # [B, r3]
out = U_o(fused)     # [B, d_out]
```

------



