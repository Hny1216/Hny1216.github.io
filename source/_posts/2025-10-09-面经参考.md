---
title: 面经参考
math: true
fancybox: true
date: 2025-10-09
categories:
- 技能工具

---

面经参考

<!-- more -->

# 面经参考



## Transformer

##### Transformer 中的可训练 Queries、Keys 和 Values 矩阵从哪儿来？Transformer 中为何会有 Queries、Keys 和 Values 矩阵，只设置 Values 矩阵本身来求 Attention 不是更简单吗？

+++info 

Q、K、V矩阵并非凭空而来，而是通过可学习的线性变换从输入数据中派生出来的。它们的分工合作是Transformer理解复杂依赖关系的精髓所在：

- **Q（查询）** 负责主动“提问”，寻找相关信息。
- **K（键）** 负责被动“应答”，提供可被匹配的标识。
- **V（值）** 负责承载“实质内容”，是被最终提取的信息。

这种“**各司其职**”的设计，使得Transformer能够动态地、有区分地聚焦于输入序列中不同部分的信息，这是其强大表达能力的基础。如果只保留V矩阵，就等于放弃了这种灵活且强大的注意力筛选机制。

+++

##### Transformer 的 Feed Forward 层在训练的时候到底在训练什么？

+++info

Transformer的Feed Forward层（FFN）在训练时，核心目标是学习如何对自注意力层输出的、已经融合了上下文信息的向量表示进行更深层次的**非线性变换和特征加工**。

通常由一个“**升维-激活-降维**”的流程构成。

+++

##### Transformer 的 Positional Encoding 是如何表达相对位置关系的，位置信息在不同的Encoder 的之间传递会丢失吗？

+++info

利用经典正弦余弦编码的相对性原理，原版Transformer采用的正弦余弦编码公式如下：

$$PE(pos,2i)=sin(10000^{2i/d_{pos}})$$

$$PE(pos,2i+1)=cos(10000^{2i/d_{pos}})$$

其中 *pos*是绝对位置，*i*是维度索引，*d*是模型维度。

位置信息会**贯穿整个模型的深层，在不同Encoder层之间稳定传递**。这主要得益于Transformer的两项核心设计：

1. **残差连接**：每个子层（自注意力层、前馈网络）的输出都包含其输入信息，而输入中已经融入了位置编码。通过残差连接，这些位置信息得以直接传递到下一层。
2. **逐层传递**：每一层的输出都作为下一层的输入。因此，经过第一层Encoder处理后得到的表征，已经是一个**融合了词义和位置信息的复合体**。这个复合体作为输入进入第二层Encoder，其内部包含的位置信息会继续参与第二层的计算，并影响其输出。位置编码的影响会随着数据在各层中的前向传播而持续下去。

+++

##### Transformer 中的 Layer Normalization 蕴含的神经网络的假设是什么？为何使用Layer Norm 而不是 Batch Norm？Transformer 是否有其它更好的 Normalization 的实现？

+++info 

LayerNorm的核心思想是**对单个样本在某一层所有神经元节点的输出进行归一化**。它基于一个重要的假设：**一个样本在神经网络某一层中的所有特征维度的分布应该是稳定且相似的**（可以理解为满足某种独立同分布特性）。

Transformer选择LayerNorm而非BatchNorm，主要基于以下几点考虑：

1. **对变长序列的天然适应性**：自然语言处理中，句子长度各不相同。为了批量处理，短句会被填充（Padding）到相同长度。LayerNorm**独立处理每个样本**，完全不受批次内其他序列长度或填充位置的影响。而BatchNorm会计算批次内所有样本同一特征的均值和方差，填充位的零值会严重污染这些统计量，导致归一化效果不佳。
2. **训练稳定性和批次大小不敏感**：Transformer模型庞大，训练时有时会采用较小的批次大小（Mini-batch）。BatchNorm的效果高度依赖于批次大小，小批次时其计算的统计量非常不准确。LayerNorm的计算与批次大小无关，因此在各种规模的批次下都能提供稳定的归一化效果，保证了训练过程的稳健。
3. **与自注意力机制的并行计算模式匹配**：Transformer的自注意力机制是高度并行化的。LayerNorm在每个样本上独立操作，非常适合这种并行计算架构，不会引入跨样本的依赖。而BatchNorm需要同步整个批次的统计信息，在并行计算环境中可能成为瓶颈。

+++

##### 请具体分析 Transformer 的 Embeddigns 层、Attention 层和 Feedforward 层的复杂度

+++info

**1. Embeddings 层**

这一层负责将离散的token ID转换为连续的向量表示。

- **时间复杂度 O(n⋅d)**：对于长度为 *n*的序列，需要执行 *n*次查表操作，每次获取一个 *d*维的向量。这是一个线性操作，效率很高。
- **空间复杂度 O(∣V∣⋅d)**：需要存储一个大小为 ∣*V*∣×*d*的嵌入矩阵，其中 ∣*V*∣是词表大小。在词汇表很大时，这部分参数会占据相当可观的内存。

**2. Attention 层**

自注意力机制是Transformer的核心，也是计算开销最大的部分。

- **时间复杂度 O($n^2$⋅d)**：这源于计算注意力得分矩阵 *$QK^T$*的步骤，该矩阵大小为 *n*×*n*。对于序列中的每个位置，都需要与其他所有位置计算关联度，因此复杂度是序列长度的平方次。这使得Transformer在处理长序列时（如长文档或高分辨率图像分块）计算成本急剧上升。
- **空间复杂度 O($d^2$)**：主要来自生成Q、K、V的三个线性变换矩阵 *W**Q*,*W**K*,*W**V*，每个矩阵的大小是 *d*×*d**k*（通常 *d**k*=*d*）。相比之下，存储 *n*×*n*的注意力矩阵本身是临时的空间开销。

**3. Feedforward 层**

前馈网络独立地处理每个位置的表示。

- **时间复杂度 O(n⋅$d^2$)**：FFN通常先将 *d*维的输入投影到一个更高的维度 *d**ff*（例如 4*d*），再投影回 *d*维。每个位置的计算复杂度是 *O*(*d*⋅*d**ff*)，对于 *n*个位置就是 *O*(*n*⋅*d*⋅*d**ff*)，即 *O*(*n*⋅*d*2)。
- **空间复杂度 \*O\*(\*d\*2)**：由两层全连接层的权重矩阵决定，即 *W*1∈R*d*×*d**ff*和 *W*2∈R*d**ff*×*d*。

+++

##### Transformer 中的神经网络为何能够很好的表示信息？

+++info

Transformer神经网络强大的信息表示能力源于其**对全局上下文的动态感知**、**并行高效的计算架构**以及**通过深度堆叠实现的层次化抽象**。它通过自注意力机制捕捉内部关联，通过位置编码理解顺序，通过前馈网络进行非线性变换，再通过残差连接和层归一化稳定地训练所有这些组件，从而使其能够学习到数据中极其复杂和抽象的模式。

+++

##### 请描述 Transformer 中的 Tokenization 的数学原理、运行流程、问题及具体改进方法

+++info 

Tokenization 的数学本质是**将连续文本离散化**，通过构建一个从文本片段到整数索引的映射函数。其核心在于**词汇表的构建算法**。



+++

##### 请描述一下你认为的把 self-attention 复杂度从 O(n2) 降低到 O(n)有效方案.

+++info

1. 低秩近似

这类方法基于一个关键观察：**自注意力矩阵往往是低秩的**，即其信息可以由一个低维子空间有效捕获。

- **Linformer**: 提出将原始的键（K）和值（V）矩阵（维度为 n×d）通过**投影矩阵**降维到 k×d（k << n）。这使得计算注意力权重时的矩阵乘法变为 (n×d) · (d×k) = n×k，从而将复杂度降至 O(n)。关键在于，即使 k 远小于 n（如 256），也能在多项任务上达到与标准注意力相当的性能。

2. 线性注意力 (Linear Attention)

这类方法通过**数学变换（核函数技巧）** 改变计算顺序，避免计算 n×n 的注意力矩阵。

- **Performer**: 使用一个**核函数**来近似标准的 softmax 注意力。其巧妙之处在于将计算重新表述为：首先计算键和值的某种特征映射的外积，然后用查询向量去乘这个结果。这样就将计算顺序从 (Q·Kᵀ)·V 转换为 Q·(Kᵀ·V)，从而将复杂度从 O(n²d) 降为 O(nd²)（当 d 为常数时，即为 O(n)）。

+++

##### 请分享一下至少三种提升 Transformer 预测速度的具体的方法及其数学原理

+++info 

1. 高效注意力机制：利用核函数对标准的 Softmax 注意力进行数学上的近似改写。通过改变计算顺序，先将键（K）和值（V）相乘，再与查询（Q）作用，从而将复杂度降至 *O*(*n*)。
2. 模型量化：将模型参数（权重）和激活值从 32 位浮点数（FP32）转换为低精度整数（如 INT8 甚至 INT4），从而带来多方面的加速效益。
3. 改进解码策略

+++

