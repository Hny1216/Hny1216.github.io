---
title: 机器学习
math: true
fancybox: true
date: 2025-12-15
categories:
- 技能工具

---

机器学习

<!-- more -->

# 机器学习

### 分类大纲

| **学习类型**            | **主要用途**                        | **核心模型/算法**                                          | **常见应用场景**                                |
| ----------------------- | ----------------------------------- | ---------------------------------------------------------- | ----------------------------------------------- |
| **监督学习**            | **分类** (Classification)           | 1. **逻辑回归** (Logistic Regression, LR)                  | 二分类问题，如邮件是否为垃圾邮件                |
|                         |                                     | 2. **支持向量机** (Support Vector Machine, SVM)            | 小样本、高维度的分类问题                        |
|                         |                                     | 3. **决策树** (Decision Tree, DT)                          | 可解释性要求高的分类和回归                      |
|                         |                                     | 4. **K-近邻算法** (K-Nearest Neighbors, KNN)               | 简单的分类和回归任务                            |
|                         |                                     | 5. **朴素贝叶斯** (Naive Bayes, NB)                        | 文本分类、情感分析                              |
|                         | **回归** (Regression)               | 1. **线性回归** (Linear Regression)                        | 预测连续值，如房价预测                          |
|                         |                                     | 2. **多项式回归** (Polynomial Regression)                  | 解决非线性关系回归问题                          |
| **集成学习** (Ensemble) | **分类/回归**                       | 1. **随机森林** (Random Forest, RF)                        | 防止过拟合、特征重要性分析                      |
|                         |                                     | 2. **梯度提升** (Gradient Boosting, GB)                    | 常用：**XGBoost**、**LightGBM**、**CatBoost**   |
|                         |                                     | 3. **Adaboost** (Adaptive Boosting)                        | 提升弱分类器的性能                              |
| **无监督学习**          | **聚类** (Clustering)               | 1. **K-均值** (K-Means)                                    | 市场细分、图像分割                              |
|                         |                                     | 2. **DBSCAN** (Density-Based Spatial Clustering)           | 发现任意形状的簇、识别异常点                    |
|                         |                                     | 3. **层次聚类** (Hierarchical Clustering)                  | 基因序列分析、构建分类树                        |
|                         | **降维** (Dimensionality Reduction) | 1. **主成分分析** (Principal Component Analysis, PCA)      | 数据可视化、数据预处理                          |
|                         |                                     | 2. **T-SNE** (T-distributed Stochastic Neighbor Embedding) | 高维数据可视化                                  |
|                         | **关联规则**                        | **Apriori** (先验算法)                                     | 购物篮分析、推荐系统                            |
| **深度学习**            | **感知机/神经网络**                 | 1. **多层感知机** (Multi-Layer Perceptron, MLP)            | 最基础的神经网络结构                            |
|                         | **图像/视觉**                       | 2. **卷积神经网络** (Convolutional Neural Network, CNN)    | 图像分类、目标检测、人脸识别                    |
|                         | **序列/文本**                       | 3. **循环神经网络** (Recurrent Neural Network, RNN)        | 文本生成、语音识别                              |
|                         |                                     | 4. **长短期记忆网络** (Long Short-Term Memory, LSTM)       | 解决 RNN 的梯度消失问题                         |
|                         |                                     | 5. **Transformer/Attention 机制**                          | **BERT, GPT** 等大模型的基石，取代 RNN 成为主流 |

### 线性回归

**核心任务**：预测一个**连续值**（例如：房价、气温、电池剩余寿命）。 它试图找到一条直线（或高维超平面）来拟合数据点。

#### 核心公式

模型假设输入特征 $x$ 和输出 $y$ 之间存在线性关系：

$$y = w^T x + b$$

#### 损失函数 (Loss Function)

线性回归通常使用 均方误差 (Mean Squared Error, MSE)。目的是最小化预测值与真实值之间差值的平方和。

$$J(w, b) = \frac{1}{2m} \sum_{i=1}^{m} (y^{(i)} - \hat{y}^{(i)})^2$$

:::info no-icon

**为什么用平方？** 因为平方可以消除正负误差的影响，并且是凸函数，方便求导和寻找全局最优解。

:::



### 逻辑回归

**核心任务**：虽然叫“回归”，但它实际上是解决 **二分类** 问题（例如：是否患病、是否点击广告）。 它预测的是事件发生的**概率**（0 到 1 之间的值）。

#### 核心公式

它在线性回归的基础上，加了一个 激活函数 (Sigmoid Function)，把输出压缩到 $(0, 1)$ 之间。

$$h_\theta(x) = \text{sigmoid}(w^T x + b) = \frac{1}{1 + e^{-(w^T x + b)}}$$

#### 损失函数 (Loss Function)

逻辑回归 不能 使用 MSE（均方误差），因为引入 Sigmoid 后，MSE 不再是凸函数，会导致梯度下降陷入局部最优解。

它使用对数损失 (Log Loss)，也叫 交叉熵损失 (Cross-Entropy Loss)：

$$J(w) = - \frac{1}{m} \sum_{i=1}^{m} [y^{(i)} \log(\hat{y}^{(i)}) + (1 - y^{(i)}) \log(1 - \hat{y}^{(i)})]$$

这个公式的物理意义是：极大似然估计 (Maximum Likelihood Estimation)。

:::info no-icon

深度思考：常见的面试坑点

1. **逻辑回归可以处理非线性分类吗？**
   - **直接用不行**，因为它是线性分类器（决策边界是直线）。
   - **但在特征工程后可以**，例如引入高维特征（$x^2, x^3$ 等），或者使用核技巧（Kernel Trick），它就可以画出曲线边界。
2. **为什么逻辑回归要用 Sigmoid？**
   - 除了将数值映射到 $(0,1)$ 代表概率外，Sigmoid 函数的导数非常漂亮：$\sigma'(z) = \sigma(z)(1 - \sigma(z))$，这让梯度计算非常简便。
3. **正则化 (Regularization)**：
   - 为了防止过拟合，这两种模型通常都会加 **L1 (Lasso)** 或 **L2 (Ridge)** 正则项。
   - **L1** 倾向于产生稀疏权重（很多 $w$ 变为 0），适合特征选择。
   - **L2** 倾向于让权重普遍变小，防止模型对单一特征过分敏感。

:::



### 支持向量机

在逻辑回归中，只要能把两类分开的线都可以。但在 SVM 中，我们追求**“最宽的街道”**。

- **超平面 (Hyperplane)**：就是那条分类的线（在高维是面）。
- **间隔 (Margin)**：由于我们希望分类器不仅能分对训练数据，还能对未知数据有好的容错性，所以我们要找到一条线，让它离最近的样本点**越远越好**。这个“最远距离”的两倍就是间隔。
- **支持向量 (Support Vectors)**：**重点！** 并非所有的样本点都对确定这条线有贡献。只有**离线最近的那几个点**（即“撑”起街道宽度的点）决定了模型。这些点叫支持向量。

#### 目标函数 (Objective Function)

我们想要最大化间隔。几何间隔的公式是 $\frac{1}{||w||}$。最大化 $\frac{1}{||w||}$ 等价于 最小化 $||w||^2$。

所以，硬间隔（Hard Margin）SVM 的原始优化目标是：

$$\min_{w,b} \frac{1}{2}||w||^2$$

$$\text{s.t.} \quad y_i(w^T x_i + b) \geq 1, \quad i=1, ..., m$$

(这里的约束条件是：所有样本点都必须被正确分类，且都在间隔之外)

#### 对偶问题求解

求解过程可以分为三个阶段：**构造拉格朗日函数** $\rightarrow$ **转化为对偶问题** $\rightarrow$ **利用 SMO 算法求解**。

##### 第一阶段：从“原问题”到“拉格朗日函数”

###### 1. 回顾原问题 (Primal Problem)

我们的目标是最小化 $w$ 的模长，同时保证分类正确：

$$\min_{w, b} \frac{1}{2} ||w||^2$$

$$\text{s.t.} \quad y_i(w^T x_i + b) \geq 1, \quad i=1, \dots, m$$

这是一个带约束的凸优化问题。为了消除约束，我们需要引入**拉格朗日乘子 (Lagrange Multipliers)** $\alpha_i$（其中 $\alpha_i \geq 0$）。

###### 2. 构造拉格朗日函数

我们将约束条件融合到目标函数中：

$$\mathcal{L}(w, b, \alpha) = \frac{1}{2}||w||^2 - \sum_{i=1}^{m} \alpha_i [y_i(w^T x_i + b) - 1]$$

- 这个函数的物理意义是：如果约束被满足，第二项为负或0，最小化 $\mathcal{L}$ 等同于原问题；如果约束被违背，第二项会变得非常大，迫使优化往满足约束的方向走。

##### 第二阶段：推导对偶形式 (The Dual Form)

原问题是“极小极大”问题（$\min_{w,b} \max_{\alpha} \mathcal{L}$）。对偶问题是它的交换形式，即“极大极小”问题（$\max_{\alpha} \min_{w,b} \mathcal{L}$）。

我们需要先**对 $w$ 和 $b$ 求偏导并令其为 0**（求极小值），消掉这两个变量。

###### 1. 求偏导

$$\frac{\partial \mathcal{L}}{\partial w} = w - \sum_{i=1}^{m} \alpha_i y_i x_i = 0 \quad \Rightarrow \quad w = \sum_{i=1}^{m} \alpha_i y_i x_i$$

$$\frac{\partial \mathcal{L}}{\partial b} = - \sum_{i=1}^{m} \alpha_i y_i = 0 \quad \Rightarrow \quad \sum_{i=1}^{m} \alpha_i y_i = 0$$

> **💡 关键点**：第一个公式 $w = \sum \alpha_i y_i x_i$ 告诉我们，**权重向量 $w$ 只是数据点 $x_i$ 的线性组合**。而且只有支持向量的 $\alpha_i > 0$，其他非支持向量的 $\alpha_i = 0$，这意味着 $w$ 只由少数支持向量决定。

###### 2. 代回拉格朗日函数

将上面求出的 $w$ 代回 $\mathcal{L}(w, b, \alpha)$ 中，经过化简（推导过程略去繁琐的代数运算），我们得到了只包含 $\alpha$ 的**对偶目标函数**：

$$\max_{\alpha} \left( \sum_{i=1}^{m} \alpha_i - \frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_i \alpha_j y_i y_j (x_i^T x_j) \right)$$

**约束条件变为：**

1. $\sum_{i=1}^{m} \alpha_i y_i = 0$
2. $\alpha_i \geq 0$ （如果是软间隔 SVM，则是 $0 \leq \alpha_i \leq C$）

> 💡 为什么这里可以用核函数？
>
> 注意公式里的 $(x_i^T x_j)$，这是两个样本的内积。
>
> 如果我们需要把数据映射到高维空间 $\phi(x)$，我们只需要把内积换成核函数 $K(x_i, x_j)$ 即可，完全不需要知道 $\phi(x)$ 具体长什么样。这就是核技巧生效的地方。



### K-近邻



### 决策树



### 随机森林



### Adaboost



### XGBoost



## 聚类

### K-均值



### DBSCAN



### 层次聚类



## 降维

### 主成分分析



### T-SNE





